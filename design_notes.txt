// Other things that we need are a software algorithm to convert a matrix into the CISR format


// Memory Handler for the Sparse Matrix and Converter to the CISR Format 

/*
Things needed:
- Data line width (how long do we want the words/do we want to fetch multiple words at a time)
- Do we need a buffer or is this handled on the NOC side 
- There are two components that will be accessing the memory
    - The Sparse Matrix Loader 
    - The vector loader 

- Cache line is 64 bytes (8 64 bit words 

- Ensure that the fetches are cache aligned otherwise the algorithm wouldn't work (requires that the matrices are aligned)
- Or you design the architecture such that if you miss on the some of the values in the cache line (ie a memory call returns less values then expected you can still align the fetch later)

- Do we want to use a large section of the RAM to store the vector instead of doing the crossbar 
- What access structure do we need if we are parallelizing the result 

- Need a physical memory address (Does this mean we need access to the memory virtualization process?)
- Or can we assume that the processor sends us the correct memory address? 


We can define the integer word size in C++ everything is assumed to be 64 bits and just broken up into smaller word sizes 


Vector Loads Independent (Spatially Local expect first cache miss and then multiple cache hits (Large Cache Words in the L2)) -> 

5 Commands recieved from the processor:
- Sparse Matrix Value Vector (Address pointer)
- Sparse Matrix Row Length (CISR) pointer 
- Sparse Matrix Vector Channel ID pointer 
- Sparse Matrix Vector Column Index pointer
- Multiplication Vector


Memory Request Backup in the Queue 
1. Assume that we are going to miss on one of the cache lines for the matrix (They are all stored locally)
2. Want another transaction ID for the next round of values stored in the CISR format (Or we can just wait for the original transaction ID to come back and stall)


*/

// Parallelized Computation Channels 

/*
We need to load the values from the sparse matrix memory into an arbriter struture that directs the vectors into whichever channel they should be in

- Things to note 
  - Need to know the memory offset for the matrixes in the physical memory address space

- We will have 4 channels starting out. (Depending on the cache line length and the number word)

4 channel ID's that the rows for the CISR will get converted for 
- Each channel has its own fifo involving an ID 


*/

// Pipeline Diagram Rough 


/*

Use buffers to segment the pipeline stages

Single Stage: (Wait until complete)
1. Load Input Vector 

Computational Pipeline: 

1. Fetch CISR Matrix element (Segemented into 4 data lines) have to wait until all three arrive (stall)
 - Value (Trans id 1)
 - Column Indice (Trans id 2)
 - Row Length (Trans id 3)

This fetch needs to be queued so that we can continuously pull the values 

2. Arbiter figures which channel to forward the values to depending on which channel is available

3. Channel Calculation (Mul 0)
 - Read the vector and forward to the appropiate channel 
 - Start Multiplication 

4. (Mul 1) // at some point insert a row id to the element to tell the accumulator which row the calculation belongs to 
5. (Mul 2) 
6. (Mul 3)

7. Move into accumulator (Sums for each row) Max # of sums being kept track of equivalent to number of channels
  - Multiple channels each keeping track of the row id conversion done inside the channel fifo
*/

/*
First in idle state.

1. Send command initialize SPMV with the SPM matrix pointer 

2. Send the number of rows and the number of non-zero elements in the SPM (To separate steps in the initialize process)
3. Send the pointer to the vector and the lenght of the vector 


4. Begin the vector prefetching before the pipeline (Cannot begin until clock cycle after recieving the pointer to the vector and the lenght)

5. Enter the pipeline 
*/